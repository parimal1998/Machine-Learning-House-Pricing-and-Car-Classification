geom_histogram(bins = 30, fill = "steelblue") +
theme_minimal()
#Checking the box plot for house price
ggplot(Valuation, aes(y = House_Price)) +
geom_boxplot(fill = "skyblue", color = "black") +
labs(title = "Boxplot of House Prices", y = "House Price") +
theme_minimal()
cor(Valuation[c("House_Age", "Dist_to_station", "No_of_stores", "Latitude", "Longitude", "House_Price")])
pairs(Valuation[c("House_Age", "Dist_to_station", "No_of_stores", "Latitude", "Longitude", "House_Price")])
ggplot(Valuation, aes(x = Dist_to_station, y = House_Price)) +
geom_point(alpha = 0.6) +
geom_smooth(method = "lm", se = FALSE) +
theme_minimal()
```{r, echo=FALSE}
library(corrplot)
corrplot(cor(Valuation[, sapply(Valuation, is.numeric)]), method = "color")
library(psych)
pairs.panels(Valuation[c("House_Age", "Dist_to_station", "No_of_stores", "Latitude", "Longitude", "House_Price")])
pairs.panels(Valuation[c("House_Age", "Dist_to_station", "No_of_stores", "Latitude", "Longitude", "House_Price")])
#Removing Row column from data set
Valuation_Cleared <- Valuation
Valuation_Cleared <- Valuation_Cleared %>% select(-Date, -Row_No, -House_price_unit)
Valuation_Cleared <- Valuation_Cleared %>% select(-Date, -Row_No, -House_price_unit)
library(dplyr)
library(ggplot2)
library(caret)
library(tidyverse)
library(rpart)
library(Matrix)
library(gmodels)
library(randomForest)
library(xgboost)
library(caret)
library(class)
library(corrplot)
library(psych)
#Functions
#Function to replace single value in a column
replace_value <- function(data, column_name, old_value, new_value) {
data <- data %>%
mutate(
{{ column_name }} := ifelse({{ column_name }} == old_value, new_value, {{ column_name }})
)
return(data)
}
count_plot <- function(df, categorical_cols) {
# Loop through the columns and plot
for (col in categorical_cols) {
print(ggplot(df, aes(x = .data[[col]])) + # Using .data for dynamic column names
geom_bar(aes(fill = .data[[col]])) +
labs(title = paste("Distribution of", col),
x = col,
y = "Count") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
scale_fill_viridis_d())
}
}
#Function for evaluation of all Classification Models
generate_crosstab <- function(actual, predicted, row_label = "Actual Default", col_label = "Predicted Default") {
CrossTable(actual, predicted,
prop.chisq = FALSE,
prop.r = FALSE,
prop.c = FALSE,
dnn = c(row_label, col_label))
}
#Evaluation Function for Regression
evaluate_model <- function(actual, predicted) {
r2 <- 1 - sum((predicted - actual)^2) / sum((actual - mean(actual))^2)
rmse_val <- rmse(actual, predicted)
mae_val <- mae(actual, predicted)
return(c(R2 = round(r2, 4), RMSE = round(rmse_val, 4), MAE = round(mae_val, 4)))
}
#Converting dates from decimal to normal format
convert_decimal_to_year_month <- function(decimal_date) {
year <- floor(decimal_date)
month_fraction <- decimal_date - year
month <- floor(month_fraction * 12) + 1
return(sprintf("%d-%02d", year, month))
}
Car <- read.csv("Car_Classification.csv", stringsAsFactors = TRUE)
head(Car)
str(Car)
library(dplyr)
#Running the function to change value
Car <- replace_value(Car, Doors, "5more", "5+")
Car <- replace_value(Car, Persons, "more", "5+")
#Finding out all the unique values in data set and checking if there is any inconsistency in the file
for (column_name in names(Car)) {
unique_values_list <- list()
# Extract the column
column <- Car[[column_name]]
# Find unique values
unique_values <- unique(column)
# Store the unique values in the list, using the column name as the key
unique_values_list[[column_name]] <- unique_values
cat(column_name, ": ", paste(unique_values, collapse = ", "), "\n")
}
blank_counts <- sapply(Car, function(x) sum(x == ""))
print(blank_counts)
library(ggplot2)
categorical_cols <- c("Buying.Price", "Maintainence", "Doors", "Persons", "Lug_Space", "Safety", "Class")
# Function to run count plot on all the columns present
count_plot(Car, c("Class"))
ggplot(Car, aes(x = Buying.Price, fill = Class)) +
geom_bar(position = "fill") +
labs(title = "Buying Price vs Car Class", y = "Proportion", x = "Buying Price") +
theme_minimal()
library(tidyverse)
library(rpart)
library(Matrix)
set.seed(123)
#Building the Train and Validation Dataset
train <- sample(nrow(Car), 0.75*nrow(Car), replace = FALSE)
TrainSet <- Car[train,]
ValidSet <- Car[-train,]
summary(TrainSet)
summary(ValidSet)
DTmodel <- rpart(Class ~ ., data = TrainSet, method = "class")
#Making Predictions based on DT Model
DTpredictions <- predict(DTmodel, ValidSet, type = "class")
#Evaluation of Model
library(gmodels)
generate_crosstab(ValidSet$Class, DTpredictions, "Actual Default", "Predicted Default")
library(randomForest)
rf_model <- randomForest(Class ~ ., data = TrainSet)
RFpredictions <- predict(rf_model, ValidSet)
#Evaluation of Model
generate_crosstab(ValidSet$Class, RFpredictions, "Actual Default", "Predicted Default")
library(xgboost)
library(caret)
dummies_XG <- dummyVars(Class ~ ., data = Car)
features_matrix <- predict(dummies_XG, newdata = Car)
# Converting to matrix as xgboost doesn't support dataframe
x_data <- as.matrix(features_matrix)
# Converting Class to numeric labels starting from 0 as xgboost doesn't take factors
y_data <- as.numeric(Car$Class) - 1
x_train <- x_data[train, ]
y_train <- y_data[train]
x_test <- x_data[-train, ]
y_test <- y_data[-train]
xgb_model <- xgboost(
data = x_train,
label = y_train,
objective = "multi:softmax",
num_class = length(unique(y_data)),
nrounds = 100,
max_depth = 4,
eta = 0.3,
verbose = 0
)
#Making Predictions on the XGboost Model
predictions <- predict(xgb_model, x_test)
# Converting numeric prediction to factor for comparison
predicted_class <- factor(predictions, labels = levels(Car$Class))
actual_class <- factor(y_test, labels = levels(Car$Class))
generate_crosstab(ValidSet$Class, predicted_class, "Actual Default", "Predicted Default")
library(class)
#Performing a loop to find out the best neighbours for KNN
k_values <- 1:45
accuracy_scores <- numeric(length(k_values))
for (i in k_values) {
knn_pred <- knn(train = x_train, test = x_test, cl = y_train, k = i)
accuracy_scores[i] <- mean(knn_pred == y_test)
}
# Find best k
best_k <- which.max(accuracy_scores)
cat("Best k =", best_k, "with Accuracy =", round(accuracy_scores[best_k] * 100, 2), "%\n")
# Plotting accuracy vs. k
accuracy_df <- data.frame(k = k_values, Accuracy = accuracy_scores)
ggplot(accuracy_df, aes(x = k, y = Accuracy)) +
geom_line(color = "steelblue", size = 1.2) +
geom_point(color = "darkred", size = 2) +
geom_vline(xintercept = best_k, linetype = "dashed", color = "forestgreen") +
scale_y_continuous(labels = scales::percent_format()) +
labs(title = "KNN Accuracy vs. K",
x = "Number of Neighbors (k)",
y = "Accuracy") +
theme_minimal()
generate_crosstab(ValidSet$Class, knn_pred, "Actual Default", "Predicted Default")
library(readxl)
Valuation <- read_excel("Valuation_Regression.xlsx")
head(Valuation)
str(Valuation)
#Checking if there is any blank row in data set
blank_counts <- sapply(Valuation, function(x) sum(x == ""))
print(blank_counts)
#Re-coding the variables for easy modelling and programming
library(dplyr)
#Changing the dates from decimal to date format
Valuation$`X1 transaction date` <- convert_decimal_to_year_month(Valuation$`X1 transaction date`)
#Changing the column names
colnames(Valuation) <- c("Row_No", "Date", "House_Age", "Dist_to_station", "No_of_stores", "Latitude", "Longitude", "House_price_unit")
#Checking the column names
colnames(Valuation)
Valuation <- Valuation %>%
filter(House_Price >= 3000 & House_Price <= 25000)
library(psych)
pairs.panels(Valuation[c("House_Age", "Dist_to_station", "No_of_stores", "Latitude", "Longitude", "House_Price")])
pairs.panels(Valuation[c("House_Age", "Dist_to_station", "No_of_stores", "Latitude", "Longitude", "House_Price")])
str(Valuation)
cor(Valuation[c("House_Age", "Dist_to_station", "No_of_stores", "Latitude", "Longitude", "House_Price")])
#Converting the house prices as currently it is in area format
Valuation$House_Price <- (Valuation$House_price_unit*1000) / 3.3
str(Valuation)
ggplot(Valuation, aes(House_Price)) +
geom_histogram(bins = 30, fill = "steelblue") +
theme_minimal()
#Checking the box plot for house price
ggplot(Valuation, aes(y = House_Price)) +
geom_boxplot(fill = "skyblue", color = "black") +
labs(title = "Boxplot of House Prices", y = "House Price") +
theme_minimal()
library(corrplot)
corrplot(cor(Valuation[, sapply(Valuation, is.numeric)]), method = "color")
cor(Valuation[c("House_Age", "Dist_to_station", "No_of_stores", "Latitude", "Longitude", "House_Price")])
ggplot(Valuation, aes(x = Dist_to_station, y = House_Price)) +
geom_point(alpha = 0.6) +
geom_smooth(method = "lm", se = FALSE) +
theme_minimal()
library(corrplot)
corrplot(cor(Valuation[, sapply(Valuation, is.numeric)]), method = "color")
cor(Valuation[c("House_Age", "Dist_to_station", "No_of_stores", "Latitude", "Longitude", "House_Price")])
#Building a pair plot to check the relations between variables
pairs(Valuation[c("House_Age", "Dist_to_station", "No_of_stores", "Latitude", "Longitude", "House_Price")])
library(psych)
pairs.panels(Valuation[c("House_Age", "Dist_to_station", "No_of_stores", "Latitude", "Longitude", "House_Price")])
#Removing Row column from data set
Valuation_Cleared <- Valuation
Valuation_Cleared <- Valuation_Cleared %>% select(-Date, -Row_No, -House_price_unit)
#Splitting the train and test data set
set.seed(123)
trainIndex <- sample(nrow(Valuation_Cleared), 0.75*nrow(Valuation_Cleared), replace = FALSE)
train <- Valuation_Cleared[trainIndex,]
test <- Valuation_Cleared[-trainIndex,]
pairs.panels(Valuation[c("House_Age", "Dist_to_station", "No_of_stores", "Latitude", "Longitude", "House_Price")])
#Building the regression Model
Regression_Model <- lm(House_Price ~ ., data = train)
Regression_Model
```{r, echo=FALSE}
#Building the regression Model
Regression_Model <- lm(House_Price ~ ., data = train)
Regression_Model
print(results)
#importing the data set into R file
library(readxl)
Valuation <- read_excel("Dataset/Valuation_Regression.xlsx")
#Detecting the structure and summary of the data set
str(Valuation)
summary(Valuation)
#Checking if there is any blank row in data set
blank_counts <- sapply(Valuation, function(x) sum(x == ""))
print(blank_counts)
#Re-coding the variables for easy modelling and programming
library(dplyr)
#Changing the dates from decimal to date format
Valuation$`X1 transaction date` <- convert_decimal_to_year_month(Valuation$`X1 transaction date`)
#Changing the column names
colnames(Valuation) <- c("Row_No", "Date", "House_Age", "Dist_to_station", "No_of_stores", "Latitude", "Longitude", "House_price_unit")
#Checking the column names
colnames(Valuation)
#Converting the house prices as currently it is in area format
Valuation$House_Price <- (Valuation$House_price_unit*1000) / 3.3
#Checking for some potential outliers in the data set
library(ggplot2)
ggplot(Valuation, aes(House_Price)) +
geom_histogram(bins = 30, fill = "steelblue") +
theme_minimal()
#Checking the relation between Dist to station and House Price
ggplot(Valuation, aes(x = Dist_to_station, y = House_Price)) +
geom_point(alpha = 0.6) +
geom_smooth(method = "lm", se = FALSE) +
theme_minimal()
Valuation <- Valuation %>%
filter(House_Price >= 3000 & House_Price <= 25000)
#Checking the box plot for house price
ggplot(Valuation, aes(y = House_Price)) +
geom_boxplot(fill = "skyblue", color = "black") +
labs(title = "Boxplot of House Prices", y = "House Price") +
theme_minimal()
library(corrplot)
corrplot(cor(Valuation[, sapply(Valuation, is.numeric)]), method = "color")
cor(Valuation[c("House_Age", "Dist_to_station", "No_of_stores", "Latitude", "Longitude", "House_Price")])
#Building a pair plot to check the relations between variables
pairs(Valuation[c("House_Age", "Dist_to_station", "No_of_stores", "Latitude", "Longitude", "House_Price")])
#Checking the distribution of every variable using psych
#install.packages("psych")
library(psych)
pairs.panels(Valuation[c("House_Age", "Dist_to_station", "No_of_stores", "Latitude", "Longitude", "House_Price")])
#Removing Row column from data set
Valuation_Cleared <- Valuation
Valuation_Cleared <- Valuation_Cleared %>% select(-Date, -Row_No, -House_price_unit)
#Splitting the train and test data set
set.seed(123)
trainIndex <- sample(nrow(Valuation_Cleared), 0.75*nrow(Valuation_Cleared), replace = FALSE)
train <- Valuation_Cleared[trainIndex,]
test <- Valuation_Cleared[-trainIndex,]
#Building the regression Model
Regression_Model <- lm(House_Price ~ ., data = train)
Regression_Model
summary(Regression_Model)
#Improving the performance of Model
Valuation_Cleared$House_Age2 <- Valuation_Cleared$House_Age^2
Valuation_Cleared$Dist_to_station2 <- Valuation_Cleared$Dist_to_station^2
Valuation_Cleared$Store_gt_5 <- ifelse(Valuation_Cleared$No_of_stores >= 5, 1, 0)
#Creating new test and train set for the updated model
train_improved <- Valuation_Cleared[trainIndex,]
test_improved <- Valuation_Cleared[-trainIndex,]
Regression_Model_Improved <- lm(House_Price ~ ., data = train_improved)
summary(Regression_Model_Improved)
# Predicting Original model
pred_original <- predict(Regression_Model, newdata = test)
# Predicting Improved model
pred_improved <- predict(Regression_Model_Improved, newdata = test_improved)
#Adding actual test values
actual <- test$House_Price
# Calculating the metrics for both the model
library(Metrics)
metrics_original <- evaluate_model(actual, pred_original)
metrics_improved <- evaluate_model(actual, pred_improved)
#Printing the results
results <- rbind(Original_Model = metrics_original,
Improved_Model = metrics_improved)
print(results)
#Improving the performance of Model
Valuation_Cleared$House_Age2 <- Valuation_Cleared$House_Age^2
Valuation_Cleared$Dist_to_station2 <- Valuation_Cleared$Dist_to_station^2
Valuation_Cleared$Store_gt_5 <- ifelse(Valuation_Cleared$No_of_stores >= 5, 1, 0)
#Creating new test and train set for the updated model
train_improved <- Valuation_Cleared[trainIndex,]
test_improved <- Valuation_Cleared[-trainIndex,]
str(train_improved)
(
(
(
(
(
(
```{r, echo=FALSE}
Regression_Model_Improved <- lm(House_Price ~ ., data = train_improved)
summary(Regression_Model_Improved)
# Predicting Original model
pred_original <- predict(Regression_Model, newdata = test)
# Predicting Improved model
pred_improved <- predict(Regression_Model_Improved, newdata = test_improved)
#Adding actual test values
actual <- test$House_Price
# Calculating the metrics for both the model
library(Metrics)
metrics_original <- evaluate_model(actual, pred_original)
metrics_improved <- evaluate_model(actual, pred_improved)
#Printing the results
results <- rbind(Original_Model = metrics_original,
Improved_Model = metrics_improved)
print(results)
summary(Regression_Model_Improved)
```{r, echo=FALSE}
# Predicting Original model
pred_original <- predict(Regression_Model, newdata = test)
# Predicting Improved model
pred_improved <- predict(Regression_Model_Improved, newdata = test_improved)
#Adding actual test values
actual <- test$House_Price
# Calculating the metrics for both the model
library(Metrics)
metrics_original <- evaluate_model(actual, pred_original)
metrics_improved <- evaluate_model(actual, pred_improved)
#Printing the results
results <- rbind(Original_Model = metrics_original,
Improved_Model = metrics_improved)
print(results)
#Printing the results
results <- rbind(Original_Model = metrics_original,
Improved_Model = metrics_improved)
print(results)
summary(Regression_Model)
generate_crosstab(ValidSet$Class, DTpredictions, "Actual Default", "Predicted Default")
#importing the data set into R file
Car <- read.csv("Dataset/Car_Classification.csv", stringsAsFactors = TRUE)
#Displaying basic information about the data set structure and dimensions of data set
str(Car)
dim(Car)
summary(Car)
#Re-coding the variables for easy modelling and programming
library(dplyr)
#Running the function to change value
Car <- replace_value(Car, Doors, "5more", "5+")
Car <- replace_value(Car, Persons, "more", "5+")
#Checking if there is any blank row in data set
blank_counts <- sapply(Car, function(x) sum(x == ""))
print(blank_counts)
for (column_name in names(Car)) {
unique_values_list <- list()
# Extract the column
column <- Car[[column_name]]
# Find unique values
unique_values <- unique(column)
# Store the unique values in the list, using the column name as the key
unique_values_list[[column_name]] <- unique_values
cat(column_name, ": ", paste(unique_values, collapse = ", "), "\n")
}
#Building a count chart for checking distribution of all variable
library(ggplot2)
categorical_cols <- c("Buying.Price", "Maintainence", "Doors", "Persons", "Lug_Space", "Safety", "Class")
# Function to run count plot on all the columns present
count_plot(Car, categorical_cols)
#Checking the relationship between Features and Class
ggplot(Car, aes(x = Buying.Price, fill = Class)) +
geom_bar(position = "fill") +
labs(title = "Buying Price vs Car Class", y = "Proportion", x = "Buying Price") +
theme_minimal()
#Building a Decision Tree Model
library(tidyverse)
library(rpart)
library(Matrix)
set.seed(123)
#Building the Train and Validation Dataset
train <- sample(nrow(Car), 0.75*nrow(Car), replace = FALSE)
TrainSet <- Car[train,]
ValidSet <- Car[-train,]
summary(TrainSet)
summary(ValidSet)
#Building the Decision Tree Model
DTmodel <- rpart(Class ~ ., data = TrainSet, method = "class")
print(DTmodel)
#Plotting the Model
plot(DTmodel, uniform = TRUE, main = "Decision Tree for Car Classification")
text(DTmodel, use.n = TRUE, all = TRUE, cex = 0.7, minlength = 4)
#Making Predictions based on DT Model
DTpredictions <- predict(DTmodel, ValidSet, type = "class")
#Evaluation of Model
confusionMatrix(DTpredictions, ValidSet$Class)
#Trying Random Forest classification models to check the accuracy
library(randomForest)
rf_model <- randomForest(Class ~ ., data = TrainSet)
RFpredictions <- predict(rf_model, ValidSet)
#Evaluation of Model
confusionMatrix(RFpredictions, ValidSet$Class)
library(caret)
dummies_XG <- dummyVars(Class ~ ., data = Car)
features_matrix <- predict(dummies_XG, newdata = Car)
str(features_matrix)
# Converting to matrix as xgboost doesn't support dataframe
x_data <- as.matrix(features_matrix)
str(x_data)
# Converting Class to numeric labels starting from 0 as xgboost doesn't take factors
y_data <- as.numeric(Car$Class) - 1
str(y_data)
x_train <- x_data[train, ]
y_train <- y_data[train]
x_test <- x_data[-train, ]
y_test <- y_data[-train]
#Training the XGboost model
xgb_model <- xgboost(
data = x_train,
label = y_train,
objective = "multi:softmax",
num_class = length(unique(y_data)),
nrounds = 100,
max_depth = 4,
eta = 0.3,
verbose = 0
)
#Making Predictions on the XGboost Model
predictions <- predict(xgb_model, x_test)
# Converting numeric prediction to factor for comparison
predicted_class <- factor(predictions, labels = levels(Car$Class))
actual_class <- factor(y_test, labels = levels(Car$Class))
# Evaluate performance of xgboost model
confusionMatrix(predicted_class, actual_class)
# KNN Model
library(class)
#Performing a loop to find out the best neighbours for KNN
k_values <- 1:45
accuracy_scores <- numeric(length(k_values))
for (i in k_values) {
knn_pred <- knn(train = x_train, test = x_test, cl = y_train, k = i)
accuracy_scores[i] <- mean(knn_pred == y_test)
}
# Find best k
best_k <- which.max(accuracy_scores)
cat("Best k =", best_k, "with Accuracy =", round(accuracy_scores[best_k] * 100, 2), "%\n")
# Plotting accuracy vs. k
accuracy_df <- data.frame(k = k_values, Accuracy = accuracy_scores)
ggplot(accuracy_df, aes(x = k, y = Accuracy)) +
geom_line(color = "steelblue", size = 1.2) +
geom_point(color = "darkred", size = 2) +
geom_vline(xintercept = best_k, linetype = "dashed", color = "forestgreen") +
scale_y_continuous(labels = scales::percent_format()) +
labs(title = "KNN Accuracy vs. K",
x = "Number of Neighbors (k)",
y = "Accuracy") +
theme_minimal()
#Running KNN with best accuracy of 7
knn_pred <- knn(train = x_train, test = x_test, cl = y_train, k = 7)
accuracy <- mean(knn_pred == y_test)
cat("KNN Accuracy:", round(accuracy * 100, 2), "%\n")
#Evaluating all DT models
library(gmodels)
generate_crosstab(ValidSet$Class, DTpredictions, "Actual Default", "Predicted Default")
#Making Predictions based on DT Model
DTpredictions <- predict(DTmodel, ValidSet, type = "class")
#Evaluation of Model
confusionMatrix(DTpredictions, ValidSet$Class)
#Evaluation of Model
confusionMatrix(RFpredictions, ValidSet$Class)
# Plotting accuracy vs. k
accuracy_df <- data.frame(k = k_values, Accuracy = accuracy_scores)
#Running KNN with best accuracy of 7
knn_pred <- knn(train = x_train, test = x_test, cl = y_train, k = 7)
accuracy <- mean(knn_pred == y_test)
cat("KNN Accuracy:", round(accuracy * 100, 2), "%\n")
